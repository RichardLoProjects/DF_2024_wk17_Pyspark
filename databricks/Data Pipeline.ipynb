{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d00fd73b-9249-45fb-9147-c627daaa3629",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#from pyspark.sql import Row\n",
    "from pyspark.sql.types import (\n",
    "    StructField\n",
    "    , StringType\n",
    "    , IntegerType\n",
    "    , DoubleType\n",
    "    , StructType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffef5cee-fb9c-4047-b285-aa743efde874",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    def __init__(\n",
    "            self\n",
    "            , spark:SparkSession\n",
    "        ) -> None:\n",
    "        self.spark = spark\n",
    "        self.file_paths = {\n",
    "            'test.csv':\"/FileStore/tables/test.csv\"\n",
    "            , 'train.csv':\"/FileStore/tables/train.csv\"\n",
    "            , 'Customer_Churn_Records.csv':\"/FileStore/tables/Customer_Churn_Records.csv\"\n",
    "            , 'Bank_Customer_Churn_Prediction.csv':\"/FileStore/tables/Bank_Customer_Churn_Prediction.csv\"\n",
    "            , 'Churn_Modeling.csv':\"/FileStore/tables/Churn_Modeling.csv\"\n",
    "            , 'Churn_Modelling.csv':\"/FileStore/tables/Churn_Modelling.csv\"\n",
    "            , 'Churn_Modelling-1.csv':\"/FileStore/tables/Churn_Modelling-1.csv\"\n",
    "            , 'churn.csv':\"/FileStore/tables/churn.csv\"\n",
    "        }\n",
    "    def extract(self) -> None:\n",
    "        self.spark_dataframes = []\n",
    "        self.spark_dataframes.append(self.spark.read.csv(\n",
    "            self.file_paths['test.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('id', IntegerType(), True)\n",
    "                , StructField('CustomerId', IntegerType(), True)\n",
    "                , StructField('Surname', StringType(), True)\n",
    "                , StructField('CreditScore', IntegerType(), True)\n",
    "                , StructField('Geography', StringType(), True)\n",
    "                , StructField('Gender', StringType(), True)\n",
    "                , StructField('Age', IntegerType(), True)\n",
    "                , StructField('Tenure', IntegerType(), True)\n",
    "                , StructField('Balance', DoubleType(), True)\n",
    "                , StructField('NumOfProducts', IntegerType(), True)\n",
    "                , StructField('HasCrCard', IntegerType(), True)\n",
    "                , StructField('IsActiveMember', IntegerType(), True)\n",
    "                , StructField('EstimatedSalary', DoubleType(), True)\n",
    "            ])\n",
    "        ))\n",
    "        self.spark_dataframes.append(self.spark.read.csv(\n",
    "            self.file_paths['train.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('id', IntegerType(), True)\n",
    "                , StructField('CustomerId', IntegerType(), True)\n",
    "                , StructField('Surname', StringType(), True)\n",
    "                , StructField('CreditScore', IntegerType(), True)\n",
    "                , StructField('Geography', StringType(), True)\n",
    "                , StructField('Gender', StringType(), True)\n",
    "                , StructField('Age', IntegerType(), True)\n",
    "                , StructField('Tenure', IntegerType(), True)\n",
    "                , StructField('Balance', DoubleType(), True)\n",
    "                , StructField('NumOfProducts', IntegerType(), True)\n",
    "                , StructField('HasCrCard', IntegerType(), True)\n",
    "                , StructField('IsActiveMember', IntegerType(), True)\n",
    "                , StructField('EstimatedSalary', DoubleType(), True)\n",
    "                , StructField('Exited', IntegerType(), True)\n",
    "            ])\n",
    "        ))\n",
    "        self.spark_dataframes.append(self.spark.read.csv(\n",
    "            self.file_paths['Customer_Churn_Records.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('RowNumber', IntegerType(), True)\n",
    "                , StructField('CustomerId', IntegerType(), True)\n",
    "                , StructField('Surname', StringType(), True)\n",
    "                , StructField('CreditScore', IntegerType(), True)\n",
    "                , StructField('Geography', StringType(), True)\n",
    "                , StructField('Gender', StringType(), True)\n",
    "                , StructField('Age', IntegerType(), True)\n",
    "                , StructField('Tenure', IntegerType(), True)\n",
    "                , StructField('Balance', DoubleType(), True)\n",
    "                , StructField('NumOfProducts', IntegerType(), True)\n",
    "                , StructField('HasCrCard', IntegerType(), True)\n",
    "                , StructField('IsActiveMember', IntegerType(), True)\n",
    "                , StructField('EstimatedSalary', DoubleType(), True)\n",
    "                , StructField('Exited', IntegerType(), True)\n",
    "                , StructField('Complain', IntegerType(), True)\n",
    "                , StructField('Satisfaction Score', IntegerType(), True)\n",
    "                , StructField('Card Type', StringType(), True)\n",
    "                , StructField('Point Earned', IntegerType(), True)\n",
    "            ])\n",
    "        ))\n",
    "        self.spark_dataframes.append(self.spark.read.csv(\n",
    "            self.file_paths['Bank_Customer_Churn_Prediction.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('customer_id', IntegerType(), True)\n",
    "                , StructField('credit_score', IntegerType(), True)\n",
    "                , StructField('country', StringType(), True)\n",
    "                , StructField('gender', StringType(), True)\n",
    "                , StructField('age', IntegerType(), True)\n",
    "                , StructField('tenure', IntegerType(), True)\n",
    "                , StructField('balance', DoubleType(), True)\n",
    "                , StructField('products_number', IntegerType(), True)\n",
    "                , StructField('credit_card', IntegerType(), True)\n",
    "                , StructField('active_member', IntegerType(), True)\n",
    "                , StructField('estimated_salary', DoubleType(), True)\n",
    "                , StructField('churn', IntegerType(), True)\n",
    "            ])\n",
    "        ))\n",
    "        self.spark_dataframes.append(self.spark.read.csv(\n",
    "            self.file_paths['Churn_Modeling.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('RowNumber', IntegerType(), True)\n",
    "                , StructField('CustomerId', IntegerType(), True)\n",
    "                , StructField('Surname', StringType(), True)\n",
    "                , StructField('CreditScore', IntegerType(), True)\n",
    "                , StructField('Geography', StringType(), True)\n",
    "                , StructField('Gender', StringType(), True)\n",
    "                , StructField('Age', IntegerType(), True)\n",
    "                , StructField('Tenure', IntegerType(), True)\n",
    "                , StructField('Balance', DoubleType(), True)\n",
    "                , StructField('NumOfProducts', IntegerType(), True)\n",
    "                , StructField('HasCrCard', IntegerType(), True)\n",
    "                , StructField('IsActiveMember', IntegerType(), True)\n",
    "                , StructField('EstimatedSalary', DoubleType(), True)\n",
    "                , StructField('Exited', IntegerType(), True)\n",
    "            ])\n",
    "        ))\n",
    "        self.spark_dataframes.append(self.spark.read.csv(\n",
    "            self.file_paths['Churn_Modelling.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('RowNumber', IntegerType(), True)\n",
    "                , StructField('CustomerId', IntegerType(), True)\n",
    "                , StructField('Surname', StringType(), True)\n",
    "                , StructField('CreditScore', IntegerType(), True)\n",
    "                , StructField('Geography', StringType(), True)\n",
    "                , StructField('Gender', StringType(), True)\n",
    "                , StructField('Age', IntegerType(), True)\n",
    "                , StructField('Tenure', IntegerType(), True)\n",
    "                , StructField('Balance', DoubleType(), True)\n",
    "                , StructField('NumOfProducts', IntegerType(), True)\n",
    "                , StructField('HasCrCard', IntegerType(), True)\n",
    "                , StructField('IsActiveMember', IntegerType(), True)\n",
    "                , StructField('EstimatedSalary', DoubleType(), True)\n",
    "                , StructField('Exited', IntegerType(), True)\n",
    "            ])\n",
    "        ))\n",
    "        self.spark_dataframes.append(self.spark.read.csv(\n",
    "            self.file_paths['Churn_Modelling-1.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('RowNumber', IntegerType(), True)\n",
    "                , StructField('CustomerId', IntegerType(), True)\n",
    "                , StructField('Surname', StringType(), True)\n",
    "                , StructField('CreditScore', IntegerType(), True)\n",
    "                , StructField('Geography', StringType(), True)\n",
    "                , StructField('Gender', StringType(), True)\n",
    "                , StructField('Age', IntegerType(), True)\n",
    "                , StructField('Tenure', IntegerType(), True)\n",
    "                , StructField('Balance', DoubleType(), True)\n",
    "                , StructField('NumOfProducts', IntegerType(), True)\n",
    "                , StructField('HasCrCard', IntegerType(), True)\n",
    "                , StructField('IsActiveMember', IntegerType(), True)\n",
    "                , StructField('EstimatedSalary', DoubleType(), True)\n",
    "                , StructField('Exited', IntegerType(), True)\n",
    "            ])\n",
    "        ))\n",
    "        self.spark_dataframes.append(self.spark.read.csv(\n",
    "            self.file_paths['churn.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('RowNumber', IntegerType(), True)\n",
    "                , StructField('CustomerId', IntegerType(), True)\n",
    "                , StructField('Surname', StringType(), True)\n",
    "                , StructField('CreditScore', IntegerType(), True)\n",
    "                , StructField('Geography', StringType(), True)\n",
    "                , StructField('Gender', StringType(), True)\n",
    "                , StructField('Age', IntegerType(), True)\n",
    "                , StructField('Tenure', IntegerType(), True)\n",
    "                , StructField('Balance', DoubleType(), True)\n",
    "                , StructField('NumOfProducts', IntegerType(), True)\n",
    "                , StructField('HasCrCard', IntegerType(), True)\n",
    "                , StructField('IsActiveMember', IntegerType(), True)\n",
    "                , StructField('EstimatedSalary', DoubleType(), True)\n",
    "                , StructField('Exited', IntegerType(), True)\n",
    "            ])\n",
    "        ))\n",
    "    def transform(self) -> None:\n",
    "        ######df_renamed = df.withColumnRenamed(\"age\", \"user_age\")\n",
    "        #new_column_names = [\"first_name\", \"user_age\", \"residence\"]\n",
    "        #df_renamed = df.toDF(*new_column_names)\n",
    "        self.spark_dataframes[1].select('id', 'CustomerId').show(10)\n",
    "        self.spark_dataframes[6].select('RowNumber', 'CustomerId').show(10)\n",
    "    def load(self) -> None:\n",
    "        pass\n",
    "    def run(self) -> None:\n",
    "        self.extract()\n",
    "        self.transform()\n",
    "        self.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83ef32e-96fd-420b-818b-13135d6a3065",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ignore_comment = '''\n",
    "test = use for prediction (they have no labels)\n",
    "\n",
    "rest of data merge and run 70:30 split\n",
    "\ttrain id = RowNumber (i might just drop these columns and reset them tbh)\n",
    "\t18col = same + 4more\n",
    "\tsnek_case = total rename (also they only have 12 columns) (missing rownum and surname)\n",
    " \n",
    "BASE 14 COLUMNS\n",
    "    RowNumber (id - train.csv, not in snekcase)\n",
    "    CustomerId\n",
    "    Surname (not in snekcase)\n",
    "    CreditScore\n",
    "    Geography (country)\n",
    "    Gender\n",
    "    Age\n",
    "    Tenure\n",
    "    Balance\n",
    "    NumOfProducts (products_number)\n",
    "    HasCrCard (credit_card)\n",
    "    IsActiveMember (active_member)\n",
    "    EstimatedSalary\n",
    "    Exited (churn)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b393b54-dc81-4141-a52c-90102c0ab5fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    spark = SparkSession.builder.appName(\"Data_Pipeline\").getOrCreate()\n",
    "    pipeline = DataPipeline(spark)\n",
    "    pipeline.run()\n",
    "    print('Script ran without crashing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "087fcd4e-24ee-49ff-ab75-3fc20e470ff1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n| id|CustomerId|\n+---+----------+\n|  0|  15674932|\n|  1|  15749177|\n|  2|  15694510|\n|  3|  15741417|\n|  4|  15766172|\n|  5|  15771669|\n|  6|  15692819|\n|  7|  15669611|\n|  8|  15691707|\n|  9|  15591721|\n+---+----------+\nonly showing top 10 rows\n\n+---------+----------+\n|RowNumber|CustomerId|\n+---------+----------+\n|        1|  15634602|\n|        2|  15647311|\n|        3|  15619304|\n|        4|  15701354|\n|        5|  15737888|\n|        6|  15574012|\n|        7|  15592531|\n|        8|  15656148|\n|        9|  15792365|\n|       10|  15592389|\n+---------+----------+\nonly showing top 10 rows\n\nScript ran without crashing.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
