{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9618377-65d7-4cbc-be0b-ca7fe740a103",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## libs\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession, Row, functions\n",
    "from pyspark.sql.types import (\n",
    "    StructField\n",
    "    , StringType\n",
    "    , IntegerType\n",
    "    , DoubleType\n",
    "    , BooleanType\n",
    "    , StructType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6f6cec-d956-427c-a599-af6895b26876",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## pipeline definition\n",
    "\n",
    "class DataPipeline:\n",
    "    def __init__(self, spark_session:SparkSession, debug_mode:bool=False) -> None:\n",
    "        self.spark = spark_session\n",
    "        self.debug = debug_mode\n",
    "        self.file_paths:dict = {\n",
    "            'test.csv':'/FileStore/tables/test.csv'\n",
    "            , 'train.csv':'/FileStore/tables/train.csv'\n",
    "            , 'Customer_Churn_Records.csv':'/FileStore/tables/Customer_Churn_Records.csv'\n",
    "            , 'Bank_Customer_Churn_Prediction.csv':'/FileStore/tables/Bank_Customer_Churn_Prediction.csv'\n",
    "            , 'Churn_Modeling.csv':'/FileStore/tables/Churn_Modeling.csv'\n",
    "            , 'Churn_Modelling.csv':'/FileStore/tables/Churn_Modelling.csv'\n",
    "            , 'Churn_Modelling-1.csv':'/FileStore/tables/Churn_Modelling-1.csv'\n",
    "            , 'churn.csv':'/FileStore/tables/churn.csv'\n",
    "        }\n",
    "        self.spark_dataframes = None\n",
    "        self.joined_df = None\n",
    "        self.target_columns = None\n",
    "        self.target_schema_dict = None\n",
    "        self.target_df = None\n",
    "        print('Pipeline initialised!') if self.debug else None\n",
    "    def run(self) -> None:\n",
    "        print('Starting to run pipeline...') if self.debug else None\n",
    "        self.extract()\n",
    "        self.transform()\n",
    "        self.load()\n",
    "        print('Pipeline ran successfully!') if self.debug else None\n",
    "    def extract(self) -> None:\n",
    "        self._extract()\n",
    "    def transform(self) -> None:\n",
    "        self._transform_rename_columns()\n",
    "        self._transform_assert_existence_and_unique_id()\n",
    "        self._transform_full_outer_join()\n",
    "        self._transform_create_target_schema()\n",
    "        self._transform_filter_valid_data()\n",
    "        self._transform_populate_target_df()\n",
    "    def load(self) -> None:\n",
    "        self._load()\n",
    "    def _extract(self) -> None:\n",
    "        self.spark_dataframes = dict()\n",
    "        self.spark_dataframes['test.csv'] = self.spark.read.csv(\n",
    "            self.file_paths['test.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('id', IntegerType(), True)\n",
    "                , StructField('CustomerId', IntegerType(), True)\n",
    "                , StructField('Surname', StringType(), True)\n",
    "                , StructField('CreditScore', IntegerType(), True)\n",
    "                , StructField('Geography', StringType(), True)\n",
    "                , StructField('Gender', StringType(), True)\n",
    "                , StructField('Age', IntegerType(), True)\n",
    "                , StructField('Tenure', IntegerType(), True)\n",
    "                , StructField('Balance', DoubleType(), True)\n",
    "                , StructField('NumOfProducts', IntegerType(), True)\n",
    "                , StructField('HasCrCard', IntegerType(), True)\n",
    "                , StructField('IsActiveMember', IntegerType(), True)\n",
    "                , StructField('EstimatedSalary', DoubleType(), True)\n",
    "            ])\n",
    "        )\n",
    "        self.spark_dataframes['train.csv'] = self.spark.read.csv(\n",
    "            self.file_paths['train.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('id', IntegerType(), True)\n",
    "                , StructField('CustomerId', IntegerType(), True)\n",
    "                , StructField('Surname', StringType(), True)\n",
    "                , StructField('CreditScore', IntegerType(), True)\n",
    "                , StructField('Geography', StringType(), True)\n",
    "                , StructField('Gender', StringType(), True)\n",
    "                , StructField('Age', IntegerType(), True)\n",
    "                , StructField('Tenure', IntegerType(), True)\n",
    "                , StructField('Balance', DoubleType(), True)\n",
    "                , StructField('NumOfProducts', IntegerType(), True)\n",
    "                , StructField('HasCrCard', IntegerType(), True)\n",
    "                , StructField('IsActiveMember', IntegerType(), True)\n",
    "                , StructField('EstimatedSalary', DoubleType(), True)\n",
    "                , StructField('Exited', IntegerType(), True)\n",
    "            ])\n",
    "        )\n",
    "        self.spark_dataframes['Customer_Churn_Records.csv'] = self.spark.read.csv(\n",
    "            self.file_paths['Customer_Churn_Records.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('RowNumber', IntegerType(), True)\n",
    "                , StructField('CustomerId', IntegerType(), True)\n",
    "                , StructField('Surname', StringType(), True)\n",
    "                , StructField('CreditScore', IntegerType(), True)\n",
    "                , StructField('Geography', StringType(), True)\n",
    "                , StructField('Gender', StringType(), True)\n",
    "                , StructField('Age', IntegerType(), True)\n",
    "                , StructField('Tenure', IntegerType(), True)\n",
    "                , StructField('Balance', DoubleType(), True)\n",
    "                , StructField('NumOfProducts', IntegerType(), True)\n",
    "                , StructField('HasCrCard', IntegerType(), True)\n",
    "                , StructField('IsActiveMember', IntegerType(), True)\n",
    "                , StructField('EstimatedSalary', DoubleType(), True)\n",
    "                , StructField('Exited', IntegerType(), True)\n",
    "                , StructField('Complain', IntegerType(), True)\n",
    "                , StructField('Satisfaction Score', IntegerType(), True)\n",
    "                , StructField('Card Type', StringType(), True)\n",
    "                , StructField('Point Earned', IntegerType(), True)\n",
    "            ])\n",
    "        )\n",
    "        self.spark_dataframes['Bank_Customer_Churn_Prediction.csv'] = self.spark.read.csv(\n",
    "            self.file_paths['Bank_Customer_Churn_Prediction.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('customer_id', IntegerType(), True)\n",
    "                , StructField('credit_score', IntegerType(), True)\n",
    "                , StructField('country', StringType(), True)\n",
    "                , StructField('gender', StringType(), True)\n",
    "                , StructField('age', IntegerType(), True)\n",
    "                , StructField('tenure', IntegerType(), True)\n",
    "                , StructField('balance', DoubleType(), True)\n",
    "                , StructField('products_number', IntegerType(), True)\n",
    "                , StructField('credit_card', IntegerType(), True)\n",
    "                , StructField('active_member', IntegerType(), True)\n",
    "                , StructField('estimated_salary', DoubleType(), True)\n",
    "                , StructField('churn', IntegerType(), True)\n",
    "            ])\n",
    "        )\n",
    "        self.spark_dataframes['Churn_Modeling.csv'] = self.spark.read.csv(\n",
    "            self.file_paths['Churn_Modeling.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('RowNumber', IntegerType(), True)\n",
    "                , StructField('CustomerId', IntegerType(), True)\n",
    "                , StructField('Surname', StringType(), True)\n",
    "                , StructField('CreditScore', IntegerType(), True)\n",
    "                , StructField('Geography', StringType(), True)\n",
    "                , StructField('Gender', StringType(), True)\n",
    "                , StructField('Age', IntegerType(), True)\n",
    "                , StructField('Tenure', IntegerType(), True)\n",
    "                , StructField('Balance', DoubleType(), True)\n",
    "                , StructField('NumOfProducts', IntegerType(), True)\n",
    "                , StructField('HasCrCard', IntegerType(), True)\n",
    "                , StructField('IsActiveMember', IntegerType(), True)\n",
    "                , StructField('EstimatedSalary', DoubleType(), True)\n",
    "                , StructField('Exited', IntegerType(), True)\n",
    "            ])\n",
    "        )\n",
    "        self.spark_dataframes['Churn_Modelling.csv'] = self.spark.read.csv(\n",
    "            self.file_paths['Churn_Modelling.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('RowNumber', IntegerType(), True)\n",
    "                , StructField('CustomerId', IntegerType(), True)\n",
    "                , StructField('Surname', StringType(), True)\n",
    "                , StructField('CreditScore', IntegerType(), True)\n",
    "                , StructField('Geography', StringType(), True)\n",
    "                , StructField('Gender', StringType(), True)\n",
    "                , StructField('Age', IntegerType(), True)\n",
    "                , StructField('Tenure', IntegerType(), True)\n",
    "                , StructField('Balance', DoubleType(), True)\n",
    "                , StructField('NumOfProducts', IntegerType(), True)\n",
    "                , StructField('HasCrCard', IntegerType(), True)\n",
    "                , StructField('IsActiveMember', IntegerType(), True)\n",
    "                , StructField('EstimatedSalary', DoubleType(), True)\n",
    "                , StructField('Exited', IntegerType(), True)\n",
    "            ])\n",
    "        )\n",
    "        self.spark_dataframes['Churn_Modelling-1.csv'] = self.spark.read.csv(\n",
    "            self.file_paths['Churn_Modelling-1.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('RowNumber', IntegerType(), True)\n",
    "                , StructField('CustomerId', IntegerType(), True)\n",
    "                , StructField('Surname', StringType(), True)\n",
    "                , StructField('CreditScore', IntegerType(), True)\n",
    "                , StructField('Geography', StringType(), True)\n",
    "                , StructField('Gender', StringType(), True)\n",
    "                , StructField('Age', IntegerType(), True)\n",
    "                , StructField('Tenure', IntegerType(), True)\n",
    "                , StructField('Balance', DoubleType(), True)\n",
    "                , StructField('NumOfProducts', IntegerType(), True)\n",
    "                , StructField('HasCrCard', IntegerType(), True)\n",
    "                , StructField('IsActiveMember', IntegerType(), True)\n",
    "                , StructField('EstimatedSalary', DoubleType(), True)\n",
    "                , StructField('Exited', IntegerType(), True)\n",
    "            ])\n",
    "        )\n",
    "        self.spark_dataframes['churn.csv'] = self.spark.read.csv(\n",
    "            self.file_paths['churn.csv']\n",
    "            , header=True\n",
    "            , schema=StructType(fields=[\n",
    "                StructField('RowNumber', IntegerType(), True)\n",
    "                , StructField('CustomerId', IntegerType(), True)\n",
    "                , StructField('Surname', StringType(), True)\n",
    "                , StructField('CreditScore', IntegerType(), True)\n",
    "                , StructField('Geography', StringType(), True)\n",
    "                , StructField('Gender', StringType(), True)\n",
    "                , StructField('Age', IntegerType(), True)\n",
    "                , StructField('Tenure', IntegerType(), True)\n",
    "                , StructField('Balance', DoubleType(), True)\n",
    "                , StructField('NumOfProducts', IntegerType(), True)\n",
    "                , StructField('HasCrCard', IntegerType(), True)\n",
    "                , StructField('IsActiveMember', IntegerType(), True)\n",
    "                , StructField('EstimatedSalary', DoubleType(), True)\n",
    "                , StructField('Exited', IntegerType(), True)\n",
    "            ])\n",
    "        )\n",
    "        print('Data extracted.') if self.debug else None\n",
    "    def _transform_rename_columns(self) -> None:\n",
    "        new_column_names = [\n",
    "            'id_test_csv'\n",
    "            , 'customer_id'\n",
    "            , 'surname_test_csv'\n",
    "            , 'credit_score_test_csv'\n",
    "            , 'geography_test_csv'\n",
    "            , 'gender_test_csv'\n",
    "            , 'age_test_csv'\n",
    "            , 'tenure_test_csv'\n",
    "            , 'balance_test_csv'\n",
    "            , 'product_count_test_csv'\n",
    "            , 'has_creditcard_test_csv'\n",
    "            , 'active_member_test_csv'\n",
    "            , 'estimated_salary_test_csv'\n",
    "        ]\n",
    "        self.spark_dataframes['test.csv'] = self.spark_dataframes['test.csv'].toDF(*new_column_names)\n",
    "        new_column_names = [\n",
    "            'id_train_csv'\n",
    "            , 'customer_id'\n",
    "            , 'surname_train_csv'\n",
    "            , 'credit_score_train_csv'\n",
    "            , 'geography_train_csv'\n",
    "            , 'gender_train_csv'\n",
    "            , 'age_train_csv'\n",
    "            , 'tenure_train_csv'\n",
    "            , 'balance_train_csv'\n",
    "            , 'product_count_train_csv'\n",
    "            , 'has_creditcard_train_csv'\n",
    "            , 'active_member_train_csv'\n",
    "            , 'estimated_salary_train_csv'\n",
    "            , 'churn_train_csv'\n",
    "        ]\n",
    "        self.spark_dataframes['train.csv'] = self.spark_dataframes['train.csv'].toDF(*new_column_names)\n",
    "        new_column_names = [\n",
    "            'rownum_Customer_Churn_Records_csv'\n",
    "            , 'customer_id'\n",
    "            , 'surname_Customer_Churn_Records_csv'\n",
    "            , 'credit_score_Customer_Churn_Records_csv'\n",
    "            , 'geography_Customer_Churn_Records_csv'\n",
    "            , 'gender_Customer_Churn_Records_csv'\n",
    "            , 'age_Customer_Churn_Records_csv'\n",
    "            , 'tenure_Customer_Churn_Records_csv'\n",
    "            , 'balance_Customer_Churn_Records_csv'\n",
    "            , 'product_count_Customer_Churn_Records_csv'\n",
    "            , 'has_creditcard_Customer_Churn_Records_csv'\n",
    "            , 'active_member_Customer_Churn_Records_csv'\n",
    "            , 'estimated_salary_Customer_Churn_Records_csv'\n",
    "            , 'churn_Customer_Churn_Records_csv'\n",
    "            , 'complain_Customer_Churn_Records_csv'\n",
    "            , 'satisfaction_score_Customer_Churn_Records_csv'\n",
    "            , 'card_type_Customer_Churn_Records_csv'\n",
    "            , 'points_earned_Customer_Churn_Records_csv'\n",
    "        ]\n",
    "        self.spark_dataframes['Customer_Churn_Records.csv'] = \\\n",
    "            self.spark_dataframes['Customer_Churn_Records.csv'].toDF(*new_column_names)\n",
    "        new_column_names = [\n",
    "            'customer_id'\n",
    "            , 'credit_score_Bank_Customer_Churn_Prediction_csv'\n",
    "            , 'geography_Bank_Customer_Churn_Prediction_csv'\n",
    "            , 'gender_Bank_Customer_Churn_Prediction_csv'\n",
    "            , 'age_Bank_Customer_Churn_Prediction_csv'\n",
    "            , 'tenure_Bank_Customer_Churn_Prediction_csv'\n",
    "            , 'balance_Bank_Customer_Churn_Prediction_csv'\n",
    "            , 'product_count_Bank_Customer_Churn_Prediction_csv'\n",
    "            , 'has_creditcard_Bank_Customer_Churn_Prediction_csv'\n",
    "            , 'active_member_Bank_Customer_Churn_Prediction_csv'\n",
    "            , 'estimated_salary_Bank_Customer_Churn_Prediction_csv'\n",
    "            , 'churn_Bank_Customer_Churn_Prediction_csv'\n",
    "        ]\n",
    "        self.spark_dataframes['Bank_Customer_Churn_Prediction.csv'] = \\\n",
    "            self.spark_dataframes['Bank_Customer_Churn_Prediction.csv'].toDF(*new_column_names)\n",
    "        new_column_names = [\n",
    "            'rownum_Churn_Modeling_csv'\n",
    "            , 'customer_id'\n",
    "            , 'surname_Churn_Modeling_csv'\n",
    "            , 'credit_score_Churn_Modeling_csv'\n",
    "            , 'geography_Churn_Modeling_csv'\n",
    "            , 'gender_Churn_Modeling_csv'\n",
    "            , 'age_Churn_Modeling_csv'\n",
    "            , 'tenure_Churn_Modeling_csv'\n",
    "            , 'balance_Churn_Modeling_csv'\n",
    "            , 'product_count_Churn_Modeling_csv'\n",
    "            , 'has_creditcard_Churn_Modeling_csv'\n",
    "            , 'active_member_Churn_Modeling_csv'\n",
    "            , 'estimated_salary_Churn_Modeling_csv'\n",
    "            , 'churn_Churn_Modeling_csv'\n",
    "        ]\n",
    "        self.spark_dataframes['Churn_Modeling.csv'] = \\\n",
    "            self.spark_dataframes['Churn_Modeling.csv'].toDF(*new_column_names)\n",
    "        new_column_names = [\n",
    "            'rownum_Churn_Modelling_csv'\n",
    "            , 'customer_id'\n",
    "            , 'surname_Churn_Modelling_csv'\n",
    "            , 'credit_score_Churn_Modelling_csv'\n",
    "            , 'geography_Churn_Modelling_csv'\n",
    "            , 'gender_Churn_Modelling_csv'\n",
    "            , 'age_Churn_Modelling_csv'\n",
    "            , 'tenure_Churn_Modelling_csv'\n",
    "            , 'balance_Churn_Modelling_csv'\n",
    "            , 'product_count_Churn_Modelling_csv'\n",
    "            , 'has_creditcard_Churn_Modelling_csv'\n",
    "            , 'active_member_Churn_Modelling_csv'\n",
    "            , 'estimated_salary_Churn_Modelling_csv'\n",
    "            , 'churn_Churn_Modelling_csv'\n",
    "        ]\n",
    "        self.spark_dataframes['Churn_Modelling.csv'] = \\\n",
    "            self.spark_dataframes['Churn_Modelling.csv'].toDF(*new_column_names)\n",
    "        new_column_names = [\n",
    "            'rownum_Churn_Modelling-1_csv'\n",
    "            , 'customer_id'\n",
    "            , 'surname_Churn_Modelling-1_csv'\n",
    "            , 'credit_score_Churn_Modelling-1_csv'\n",
    "            , 'geography_Churn_Modelling-1_csv'\n",
    "            , 'gender_Churn_Modelling-1_csv'\n",
    "            , 'age_Churn_Modelling-1_csv'\n",
    "            , 'tenure_Churn_Modelling-1_csv'\n",
    "            , 'balance_Churn_Modelling-1_csv'\n",
    "            , 'product_count_Churn_Modelling-1_csv'\n",
    "            , 'has_creditcard_Churn_Modelling-1_csv'\n",
    "            , 'active_member_Churn_Modelling-1_csv'\n",
    "            , 'estimated_salary_Churn_Modelling-1_csv'\n",
    "            , 'churn_Churn_Modelling-1_csv'\n",
    "        ]\n",
    "        self.spark_dataframes['Churn_Modelling-1.csv'] = \\\n",
    "            self.spark_dataframes['Churn_Modelling-1.csv'].toDF(*new_column_names)\n",
    "        new_column_names = [\n",
    "            'rownum_churn_csv'\n",
    "            , 'customer_id'\n",
    "            , 'surname_churn_csv'\n",
    "            , 'credit_score_churn_csv'\n",
    "            , 'geography_churn_csv'\n",
    "            , 'gender_churn_csv'\n",
    "            , 'age_churn_csv'\n",
    "            , 'tenure_churn_csv'\n",
    "            , 'balance_churn_csv'\n",
    "            , 'product_count_churn_csv'\n",
    "            , 'has_creditcard_churn_csv'\n",
    "            , 'active_member_churn_csv'\n",
    "            , 'estimated_salary_churn_csv'\n",
    "            , 'churn_churn_csv'\n",
    "        ]\n",
    "        self.spark_dataframes['churn.csv'] = self.spark_dataframes['churn.csv'].toDF(*new_column_names)\n",
    "        print('Columns synchronised.') if self.debug else None\n",
    "    def _transform_assert_existence_and_unique_id(self) -> None:\n",
    "        self.spark_dataframes = \\\n",
    "            {k:df.filter(df.customer_id.isNotNull()) for k,df in self.spark_dataframes.copy().items()}\n",
    "        self.spark_dataframes = \\\n",
    "            {k:df.dropDuplicates(['customer_id']) for k,df in self.spark_dataframes.copy().items()}\n",
    "        print('Asserted existence of unique IDs.') if self.debug else None\n",
    "    def _transform_full_outer_join(self) -> None:\n",
    "        ordered_df = [df for df in self.spark_dataframes.values()]\n",
    "        self.joined_df = ordered_df[0]\n",
    "        for df in ordered_df[1:]:\n",
    "            self.joined_df = self.joined_df.join(df, on='customer_id', how='full')\n",
    "        print(f'Data merged with {self.joined_df.count()} rows.') if self.debug else None\n",
    "    def _transform_create_target_schema(self) -> None:\n",
    "        target_struct = StructType(fields=[\n",
    "            StructField('customer_id', IntegerType(), True)\n",
    "            , StructField('surname', StringType(), True)\n",
    "            , StructField('credit_score', IntegerType(), True)\n",
    "            , StructField('geography', StringType(), True)\n",
    "            , StructField('gender', StringType(), True)\n",
    "            , StructField('age', IntegerType(), True)\n",
    "            , StructField('tenure', IntegerType(), True)\n",
    "            , StructField('balance', DoubleType(), True)\n",
    "            , StructField('product_count', IntegerType(), True)\n",
    "            , StructField('has_creditcard', IntegerType(), True)\n",
    "            , StructField('active_member', IntegerType(), True)\n",
    "            , StructField('estimated_salary', DoubleType(), True)\n",
    "            , StructField('complain', IntegerType(), True)\n",
    "            , StructField('satisfaction_score', IntegerType(), True)\n",
    "            , StructField('card_type', StringType(), True)\n",
    "            , StructField('points_earned', IntegerType(), True)\n",
    "            , StructField('churn', IntegerType(), True)\n",
    "        ])\n",
    "        self.target_columns = [field.name for field in target_struct.fields]\n",
    "        self.target_schema_dict = {field.name: field.dataType for field in target_struct.fields}\n",
    "        print('Created target schema.') if self.debug else None\n",
    "    def _transform_filter_valid_data(self) -> None:\n",
    "        for target_column in self.target_columns[1:]:\n",
    "            columns = [c for c in self.joined_df.columns if target_column in c]\n",
    "            expression = ', '.join([f'`{c}`' for c in columns])\n",
    "            expression = f'filter(array({expression}), x -> x IS NOT NULL)'\n",
    "            number_of_values = functions.size(functions.array_distinct(functions.expr(expression)))\n",
    "            new_column = functions.when(number_of_values>1, functions.lit(False)).otherwise(functions.lit(True))\n",
    "            self.joined_df = self.joined_df.withColumn(f'valid_{target_column}', new_column)\n",
    "        boolean_columns = [c for c in self.joined_df.columns if 'valid' in c]\n",
    "        filter_cond = (functions.col(c) == True for c in boolean_columns)\n",
    "        filter_expr = reduce(lambda x, y: x & y, filter_cond)\n",
    "        self.joined_df = self.joined_df.filter(filter_expr)\n",
    "        print('Filtered valid data.') if self.debug else None\n",
    "    def _transform_populate_target_df(self) -> None:\n",
    "        for target_column in self.target_columns[1:]:\n",
    "            uncasted_columns = [c for c in self.joined_df.columns if c.startswith(target_column)]\n",
    "            casted_columns = [functions.col(c).cast(self.target_schema_dict[target_column]) for c in uncasted_columns]\n",
    "            self.joined_df = self.joined_df.withColumn(target_column, functions.coalesce(*casted_columns))\n",
    "        final_columns = [functions.col(c) for c in self.target_columns]\n",
    "        self.target_df = self.joined_df.select(*final_columns)\n",
    "        print(f'Target dataframe populated with {self.target_df.count()} rows.') if self.debug else None\n",
    "    def _load(self) -> None:\n",
    "        self.target_df.write.mode('overwrite').option('header', 'true').csv('/FileStore/tables/pyspark_churn.csv')\n",
    "        print('Data loaded.') if self.debug else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43bf665b-d701-46ed-bf7f-a6017691ebc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## main\n",
    "\n",
    "def main() -> None:\n",
    "    spark = SparkSession.builder.appName('Data_Pipeline').getOrCreate()\n",
    "    debug_mode = True\n",
    "    pipeline = DataPipeline(spark, debug_mode)\n",
    "    pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c40901e1-88f6-4c9d-bcd1-70eb1c1b87a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline initialised!\nStarting to run pipeline...\nData extracted.\nColumns synchronised.\nAsserted existence of unique IDs.\nData merged with 28536 rows.\nCreated target schema.\nFiltered valid data.\nTarget dataframe populated with 13545 rows.\nData loaded.\nPipeline ran successfully!\n"
     ]
    }
   ],
   "source": [
    "## run script\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
