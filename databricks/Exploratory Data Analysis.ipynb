{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc590b2-7cb8-4a4e-896c-db4be1db1554",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acb1ea76-eb7d-4ffd-8010-314918c0511d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Databricks was the chosen environment to host all compute related activities so that the project is scalable for big data. We start by configuring a compute cluster and setting up this Jupyter environment with a spark session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a71aaec6-3a30-4b5e-9b82-b6226fd72a44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Import the necessary libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62fc2bfd-5d4c-44bd-8d4c-33d57c27b274",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c3711a0-16b8-4663-8d97-5a9c77e04edc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create spark session..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "904034a1-30f5-4250-8b5e-34b5754d38e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Exploratory_Data_Analysis').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99a5ccc9-e3dd-4b69-9dda-286226001582",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Upload data to `/FileStore/tables/` and run `dbutils.fs.ls(\"/FileStore/tables/\")` to list all available files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e035b78-def8-4d25-af3d-00066cbab63c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.ls(\"/FileStore/tables/\") ## Returns a list of all files in the Databricks filesystem (commented out for privacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac13bbc3-7537-47c3-8df9-dba96c5daf2e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Collect the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd550761-a756-4f85-98ee-c6b8fcf4ea3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, file_path:str) -> None:\n",
    "        self.name = file_path.split('/')[-1]\n",
    "        self.spark_df = spark.read.csv(file_path, inferSchema=True, header=True)\n",
    "    @property\n",
    "    def num_rows(self) -> int:\n",
    "        return self.spark_df.count()\n",
    "    @property\n",
    "    def num_cols(self) -> int:\n",
    "        return len(self.spark_df.columns)\n",
    "    @property\n",
    "    def columns(self) -> list[str]:\n",
    "        return self.spark_df.columns\n",
    "\n",
    "folder_path = '/FileStore/tables/'\n",
    "file_names = [\n",
    "    'test.csv'\n",
    "    , 'train.csv'\n",
    "    , 'Customer_Churn_Records.csv'\n",
    "    , 'Bank_Customer_Churn_Prediction.csv'\n",
    "    , 'Churn_Modeling.csv'\n",
    "    , 'Churn_Modelling.csv'\n",
    "    , 'Churn_Modelling-1.csv'\n",
    "    , 'churn.csv'\n",
    "]\n",
    "file_paths = [folder_path+f for f in file_names]\n",
    "df_lst = [Dataset(f) for f in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3efddb49-1674-45e8-a2ac-79fd36ead824",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 spark dataframes.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(df_lst)} spark dataframes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64b12549-0d60-4e3d-8a92-1cf873b51c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Analysing the various sizes of the datasets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c53aca71-037b-41d2-8ded-3fae050b37ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shape_df = spark.createDataFrame(\n",
    "    [Row(\n",
    "        df_index=i\n",
    "        , number_of_columns=sdf.num_cols\n",
    "        , number_of_rows=sdf.num_rows\n",
    "        , file_name=sdf.name\n",
    "    ) for i,sdf in enumerate(df_lst)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db13d69-8817-45c4-88b7-7ef188a931dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+--------------+----------------------------------+\n",
      "|df_index|number_of_columns|number_of_rows|file_name                         |\n",
      "+--------+-----------------+--------------+----------------------------------+\n",
      "|0       |13               |110023        |test.csv                          |\n",
      "|1       |14               |165034        |train.csv                         |\n",
      "|2       |18               |10000         |Customer_Churn_Records.csv        |\n",
      "|3       |12               |10000         |Bank_Customer_Churn_Prediction.csv|\n",
      "|4       |14               |10000         |Churn_Modeling.csv                |\n",
      "|5       |14               |10000         |Churn_Modelling.csv               |\n",
      "|6       |14               |10002         |Churn_Modelling-1.csv             |\n",
      "|7       |14               |10000         |churn.csv                         |\n",
      "+--------+-----------------+--------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shape_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b26dbf-53a2-4e7d-b9ac-e36f4e90ce9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are up to 18 columns.\n"
     ]
    }
   ],
   "source": [
    "max_num_cols = shape_df.agg({'number_of_columns':'max'}).collect()[0][0]\n",
    "print(f'There are up to {max_num_cols} columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adc1a15a-5a56-4466-9e7e-aa281cbcd915",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Example schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9da7f08-fe17-4ed3-bf25-c9e2b2f4d00a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- credit_score: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- balance: double (nullable = true)\n",
      " |-- products_number: integer (nullable = true)\n",
      " |-- credit_card: integer (nullable = true)\n",
      " |-- active_member: integer (nullable = true)\n",
      " |-- estimated_salary: double (nullable = true)\n",
      " |-- churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lst[3].spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca15c992-5901-4b37-8c08-eeb4ffb624b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Analysing the column names..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f25d8401-d7dd-4139-99b8-aa0f1de2ff23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "col_tuples = [[c for c in sdf.columns] for sdf in df_lst]\n",
    "for t in col_tuples:\n",
    "    while len(t) < shape_df.agg({'number_of_columns':'max'}).collect()[0][0]:\n",
    "        t.append(None)\n",
    "col_tuples = [[str(i)]+t for i,t in enumerate(col_tuples[:])]\n",
    "col_tuples = [tuple(t) for t in col_tuples[:]]\n",
    "\n",
    "column_pandas_df = spark.createDataFrame(\n",
    "    [Row(*t) for t in col_tuples]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2aa885c-9c93-47c5-bd3e-d119d7154086",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------------+-------+-----------+---------+------+-------+---------------+-----------+-------------+----------------+--------------+---------------+------+--------+------------------+---------+------------+\n",
      "| _1|         _2|          _3|     _4|         _5|       _6|    _7|     _8|             _9|        _10|          _11|             _12|           _13|            _14|   _15|     _16|               _17|      _18|         _19|\n",
      "+---+-----------+------------+-------+-----------+---------+------+-------+---------------+-----------+-------------+----------------+--------------+---------------+------+--------+------------------+---------+------------+\n",
      "|  0|         id|  CustomerId|Surname|CreditScore|Geography|Gender|    Age|         Tenure|    Balance|NumOfProducts|       HasCrCard|IsActiveMember|EstimatedSalary|  null|    null|              null|     null|        null|\n",
      "|  1|         id|  CustomerId|Surname|CreditScore|Geography|Gender|    Age|         Tenure|    Balance|NumOfProducts|       HasCrCard|IsActiveMember|EstimatedSalary|Exited|    null|              null|     null|        null|\n",
      "|  2|  RowNumber|  CustomerId|Surname|CreditScore|Geography|Gender|    Age|         Tenure|    Balance|NumOfProducts|       HasCrCard|IsActiveMember|EstimatedSalary|Exited|Complain|Satisfaction Score|Card Type|Point Earned|\n",
      "|  3|customer_id|credit_score|country|     gender|      age|tenure|balance|products_number|credit_card|active_member|estimated_salary|         churn|           null|  null|    null|              null|     null|        null|\n",
      "|  4|  RowNumber|  CustomerId|Surname|CreditScore|Geography|Gender|    Age|         Tenure|    Balance|NumOfProducts|       HasCrCard|IsActiveMember|EstimatedSalary|Exited|    null|              null|     null|        null|\n",
      "|  5|  RowNumber|  CustomerId|Surname|CreditScore|Geography|Gender|    Age|         Tenure|    Balance|NumOfProducts|       HasCrCard|IsActiveMember|EstimatedSalary|Exited|    null|              null|     null|        null|\n",
      "|  6|  RowNumber|  CustomerId|Surname|CreditScore|Geography|Gender|    Age|         Tenure|    Balance|NumOfProducts|       HasCrCard|IsActiveMember|EstimatedSalary|Exited|    null|              null|     null|        null|\n",
      "|  7|  RowNumber|  CustomerId|Surname|CreditScore|Geography|Gender|    Age|         Tenure|    Balance|NumOfProducts|       HasCrCard|IsActiveMember|EstimatedSalary|Exited|    null|              null|     null|        null|\n",
      "+---+-----------+------------+-------+-----------+---------+------+-------+---------------+-----------+-------------+----------------+--------------+---------------+------+--------+------------------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_pandas_df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871d79ac-da5d-49bd-9e19-8cc6331d7d30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What looked good in Pandas does not look good as a Spark dataframe. Approaching representation in a different angle..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3722b20-c390-40d1-89c4-dfbb0fcf1e4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_melt_df = spark.createDataFrame(\n",
    "    [Row(\n",
    "        df_index=i\n",
    "        , column_name=c\n",
    "        , file_name=sdf.name\n",
    "    ) for i,sdf in enumerate(df_lst) for c in sdf.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4906771e-c1e9-4ee5-a6a6-a60814bd4495",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: 113"
     ]
    }
   ],
   "source": [
    "column_melt_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd1f2856-fe90-445e-8f7e-a1121693e51c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+----------------------------------+\n",
      "|df_index|column_name       |file_name                         |\n",
      "+--------+------------------+----------------------------------+\n",
      "|0       |id                |test.csv                          |\n",
      "|0       |CustomerId        |test.csv                          |\n",
      "|0       |Surname           |test.csv                          |\n",
      "|0       |CreditScore       |test.csv                          |\n",
      "|0       |Geography         |test.csv                          |\n",
      "|0       |Gender            |test.csv                          |\n",
      "|0       |Age               |test.csv                          |\n",
      "|0       |Tenure            |test.csv                          |\n",
      "|0       |Balance           |test.csv                          |\n",
      "|0       |NumOfProducts     |test.csv                          |\n",
      "|0       |HasCrCard         |test.csv                          |\n",
      "|0       |IsActiveMember    |test.csv                          |\n",
      "|0       |EstimatedSalary   |test.csv                          |\n",
      "|1       |id                |train.csv                         |\n",
      "|1       |CustomerId        |train.csv                         |\n",
      "|1       |Surname           |train.csv                         |\n",
      "|1       |CreditScore       |train.csv                         |\n",
      "|1       |Geography         |train.csv                         |\n",
      "|1       |Gender            |train.csv                         |\n",
      "|1       |Age               |train.csv                         |\n",
      "|1       |Tenure            |train.csv                         |\n",
      "|1       |Balance           |train.csv                         |\n",
      "|1       |NumOfProducts     |train.csv                         |\n",
      "|1       |HasCrCard         |train.csv                         |\n",
      "|1       |IsActiveMember    |train.csv                         |\n",
      "|1       |EstimatedSalary   |train.csv                         |\n",
      "|1       |Exited            |train.csv                         |\n",
      "|2       |RowNumber         |Customer_Churn_Records.csv        |\n",
      "|2       |CustomerId        |Customer_Churn_Records.csv        |\n",
      "|2       |Surname           |Customer_Churn_Records.csv        |\n",
      "|2       |CreditScore       |Customer_Churn_Records.csv        |\n",
      "|2       |Geography         |Customer_Churn_Records.csv        |\n",
      "|2       |Gender            |Customer_Churn_Records.csv        |\n",
      "|2       |Age               |Customer_Churn_Records.csv        |\n",
      "|2       |Tenure            |Customer_Churn_Records.csv        |\n",
      "|2       |Balance           |Customer_Churn_Records.csv        |\n",
      "|2       |NumOfProducts     |Customer_Churn_Records.csv        |\n",
      "|2       |HasCrCard         |Customer_Churn_Records.csv        |\n",
      "|2       |IsActiveMember    |Customer_Churn_Records.csv        |\n",
      "|2       |EstimatedSalary   |Customer_Churn_Records.csv        |\n",
      "|2       |Exited            |Customer_Churn_Records.csv        |\n",
      "|2       |Complain          |Customer_Churn_Records.csv        |\n",
      "|2       |Satisfaction Score|Customer_Churn_Records.csv        |\n",
      "|2       |Card Type         |Customer_Churn_Records.csv        |\n",
      "|2       |Point Earned      |Customer_Churn_Records.csv        |\n",
      "|3       |customer_id       |Bank_Customer_Churn_Prediction.csv|\n",
      "|3       |credit_score      |Bank_Customer_Churn_Prediction.csv|\n",
      "|3       |country           |Bank_Customer_Churn_Prediction.csv|\n",
      "|3       |gender            |Bank_Customer_Churn_Prediction.csv|\n",
      "|3       |age               |Bank_Customer_Churn_Prediction.csv|\n",
      "|3       |tenure            |Bank_Customer_Churn_Prediction.csv|\n",
      "|3       |balance           |Bank_Customer_Churn_Prediction.csv|\n",
      "|3       |products_number   |Bank_Customer_Churn_Prediction.csv|\n",
      "|3       |credit_card       |Bank_Customer_Churn_Prediction.csv|\n",
      "|3       |active_member     |Bank_Customer_Churn_Prediction.csv|\n",
      "|3       |estimated_salary  |Bank_Customer_Churn_Prediction.csv|\n",
      "|3       |churn             |Bank_Customer_Churn_Prediction.csv|\n",
      "|4       |RowNumber         |Churn_Modeling.csv                |\n",
      "|4       |CustomerId        |Churn_Modeling.csv                |\n",
      "|4       |Surname           |Churn_Modeling.csv                |\n",
      "|4       |CreditScore       |Churn_Modeling.csv                |\n",
      "|4       |Geography         |Churn_Modeling.csv                |\n",
      "|4       |Gender            |Churn_Modeling.csv                |\n",
      "|4       |Age               |Churn_Modeling.csv                |\n",
      "|4       |Tenure            |Churn_Modeling.csv                |\n",
      "|4       |Balance           |Churn_Modeling.csv                |\n",
      "|4       |NumOfProducts     |Churn_Modeling.csv                |\n",
      "|4       |HasCrCard         |Churn_Modeling.csv                |\n",
      "|4       |IsActiveMember    |Churn_Modeling.csv                |\n",
      "|4       |EstimatedSalary   |Churn_Modeling.csv                |\n",
      "|4       |Exited            |Churn_Modeling.csv                |\n",
      "|5       |RowNumber         |Churn_Modelling.csv               |\n",
      "|5       |CustomerId        |Churn_Modelling.csv               |\n",
      "|5       |Surname           |Churn_Modelling.csv               |\n",
      "|5       |CreditScore       |Churn_Modelling.csv               |\n",
      "|5       |Geography         |Churn_Modelling.csv               |\n",
      "|5       |Gender            |Churn_Modelling.csv               |\n",
      "|5       |Age               |Churn_Modelling.csv               |\n",
      "|5       |Tenure            |Churn_Modelling.csv               |\n",
      "|5       |Balance           |Churn_Modelling.csv               |\n",
      "|5       |NumOfProducts     |Churn_Modelling.csv               |\n",
      "|5       |HasCrCard         |Churn_Modelling.csv               |\n",
      "|5       |IsActiveMember    |Churn_Modelling.csv               |\n",
      "|5       |EstimatedSalary   |Churn_Modelling.csv               |\n",
      "|5       |Exited            |Churn_Modelling.csv               |\n",
      "|6       |RowNumber         |Churn_Modelling-1.csv             |\n",
      "|6       |CustomerId        |Churn_Modelling-1.csv             |\n",
      "|6       |Surname           |Churn_Modelling-1.csv             |\n",
      "|6       |CreditScore       |Churn_Modelling-1.csv             |\n",
      "|6       |Geography         |Churn_Modelling-1.csv             |\n",
      "|6       |Gender            |Churn_Modelling-1.csv             |\n",
      "|6       |Age               |Churn_Modelling-1.csv             |\n",
      "|6       |Tenure            |Churn_Modelling-1.csv             |\n",
      "|6       |Balance           |Churn_Modelling-1.csv             |\n",
      "|6       |NumOfProducts     |Churn_Modelling-1.csv             |\n",
      "|6       |HasCrCard         |Churn_Modelling-1.csv             |\n",
      "|6       |IsActiveMember    |Churn_Modelling-1.csv             |\n",
      "|6       |EstimatedSalary   |Churn_Modelling-1.csv             |\n",
      "|6       |Exited            |Churn_Modelling-1.csv             |\n",
      "|7       |RowNumber         |churn.csv                         |\n",
      "|7       |CustomerId        |churn.csv                         |\n",
      "|7       |Surname           |churn.csv                         |\n",
      "|7       |CreditScore       |churn.csv                         |\n",
      "|7       |Geography         |churn.csv                         |\n",
      "|7       |Gender            |churn.csv                         |\n",
      "|7       |Age               |churn.csv                         |\n",
      "|7       |Tenure            |churn.csv                         |\n",
      "|7       |Balance           |churn.csv                         |\n",
      "|7       |NumOfProducts     |churn.csv                         |\n",
      "|7       |HasCrCard         |churn.csv                         |\n",
      "|7       |IsActiveMember    |churn.csv                         |\n",
      "|7       |EstimatedSalary   |churn.csv                         |\n",
      "|7       |Exited            |churn.csv                         |\n",
      "+--------+------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_melt_df.show(113, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad7f4b8d-0394-4818-bf66-6caa98d3b61f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: 31"
     ]
    }
   ],
   "source": [
    "column_melt_df.select('column_name').dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d30843bf-5689-4128-bfc4-c8feea5dbcb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       column_name|\n",
      "+------------------+\n",
      "|     NumOfProducts|\n",
      "|           Balance|\n",
      "|       CreditScore|\n",
      "|               Age|\n",
      "|        CustomerId|\n",
      "|         HasCrCard|\n",
      "|         Geography|\n",
      "|            Tenure|\n",
      "|           Surname|\n",
      "|            Gender|\n",
      "|    IsActiveMember|\n",
      "|                id|\n",
      "|   EstimatedSalary|\n",
      "|         RowNumber|\n",
      "|            Exited|\n",
      "|          Complain|\n",
      "|     active_member|\n",
      "|      credit_score|\n",
      "|           balance|\n",
      "|         Card Type|\n",
      "|      Point Earned|\n",
      "|Satisfaction Score|\n",
      "|       customer_id|\n",
      "|            tenure|\n",
      "|       credit_card|\n",
      "|           country|\n",
      "|               age|\n",
      "|            gender|\n",
      "|   products_number|\n",
      "|  estimated_salary|\n",
      "|             churn|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_melt_df.select('column_name').dropDuplicates().show(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85505d67-40f7-4a00-9d5f-aca1e9c239c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Given that there were up to 18 columns in `shape_df.show(truncate=False)`, 31 columns in `column_df.count()` seems a bit much. The line `column_df.select('column_name').dropDuplicates().show(31)` confirms this when we see duplicate column names such as \"age\" and \"Age\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffd2d2a4-73d6-4b9b-b807-8ab2d461cd7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------------+\n",
      "|df_index|column_name|file_name                 |\n",
      "+--------+-----------+--------------------------+\n",
      "|0       |Geography  |test.csv                  |\n",
      "|1       |Geography  |train.csv                 |\n",
      "|2       |Geography  |Customer_Churn_Records.csv|\n",
      "|4       |Geography  |Churn_Modeling.csv        |\n",
      "|5       |Geography  |Churn_Modelling.csv       |\n",
      "|6       |Geography  |Churn_Modelling-1.csv     |\n",
      "|7       |Geography  |churn.csv                 |\n",
      "+--------+-----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_melt_df.filter(\"column_name = 'Geography'\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7043d5-e92e-4d48-845d-05e49b2a52aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A lot of the datasets appear to be duplicates. Selecting the first row from each dataset yields..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48a5ceb3-9f08-4c4b-8ed0-48b52ea88c4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([165034, 15773898, 'Lucchese', 586, 'France', 'Female', 23.0, 2, 0.0, 2, 0.0, 1.0, 160976.75])\n",
      "dict_values([0, 15674932, 'Okwudilichukwu', 668, 'France', 'Male', 33.0, 3, 0.0, 2, 1.0, 0.0, 181449.97, 0])\n",
      "dict_values([1, 15634602, 'Hargrave', 619, 'France', 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88, 1, 1, 2, 'DIAMOND', 464])\n",
      "dict_values([15634602, 619, 'France', 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88, 1])\n",
      "dict_values([1, 15634602, 'Hargrave', 619, 'France', 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88, 1])\n",
      "dict_values([1, 15634602, 'Hargrave', 619, 'France', 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88, 1])\n",
      "dict_values([1, 15634602, 'Hargrave', 619, 'France', 'Female', 42.0, 2, 0.0, 1, 1, 1, 101348.88, 1])\n",
      "dict_values([1, 15634602, 'Hargrave', 619, 'France', 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88, 1])\n"
     ]
    }
   ],
   "source": [
    "for sdf in df_lst:\n",
    "    print(sdf.spark_df.first().asDict().values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08cd4794-ad07-4ccb-9d63-db764a6e0271",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Setting aside `train.csv` and `test.csv`, because clearly this data has been shuffled for ML purposes... It appears that the rest of the data seems to all be the same data. Remember each dataset had approx 10,000 rows and it can be clearly seen from the first row of each dataset that they all represent the same female customer from France with credit score 619 etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "108d729e-44d8-4678-b8bf-4b13ab0aa4d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In conclusion, there may only be 2 groups of data instead of 7 groups of data. Duplicate data may come in handy when validating data during the integration process in the upcoming data and ML pipelines..."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Exploratory Data Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
