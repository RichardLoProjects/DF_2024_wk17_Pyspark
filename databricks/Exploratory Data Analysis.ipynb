{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc590b2-7cb8-4a4e-896c-db4be1db1554",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acb1ea76-eb7d-4ffd-8010-314918c0511d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Databricks was the chosen environment to host all compute related activities so that the project is scalable for big data. We start by configuring a compute cluster and setting up this Jupyter environment with a spark session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a71aaec6-3a30-4b5e-9b82-b6226fd72a44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Import the necessary libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62fc2bfd-5d4c-44bd-8d4c-33d57c27b274",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c3711a0-16b8-4663-8d97-5a9c77e04edc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create spark session..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "904034a1-30f5-4250-8b5e-34b5754d38e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Exploratory_Data_Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99a5ccc9-e3dd-4b69-9dda-286226001582",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Upload data to `/FileStore/tables/` and run `dbutils.fs.ls(\"/FileStore/tables/\")` to list all available files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e035b78-def8-4d25-af3d-00066cbab63c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.ls(\"/FileStore/tables/\") ## Returns a list of all files in the Databricks filesystem (commented out for privacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac13bbc3-7537-47c3-8df9-dba96c5daf2e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Collect the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd550761-a756-4f85-98ee-c6b8fcf4ea3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        self.name = file_path.split(\"/\")[-1]\n",
    "        self.spark_df = spark.read.csv(file_path, inferSchema=True, header=True)\n",
    "    @property\n",
    "    def num_rows(self) -> int:\n",
    "        return self.spark_df.count()\n",
    "    @property\n",
    "    def num_cols(self) -> int:\n",
    "        return len(self.spark_df.columns)\n",
    "    @property\n",
    "    def columns(self) -> list[str]:\n",
    "        return self.spark_df.columns\n",
    "\n",
    "\n",
    "folder_path = \"/FileStore/tables/\"\n",
    "file_names = [\n",
    "    \"test.csv\",\n",
    "    \"train.csv\",\n",
    "    \"Customer_Churn_Records.csv\",\n",
    "    \"Bank_Customer_Churn_Prediction.csv\",\n",
    "    \"Churn_Modeling.csv\",\n",
    "    \"Churn_Modelling.csv\",\n",
    "    \"Churn_Modelling-1.csv\",\n",
    "    \"churn.csv\",\n",
    "]\n",
    "file_paths = [folder_path + f for f in file_names]\n",
    "df_lst = [Dataset(f) for f in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3efddb49-1674-45e8-a2ac-79fd36ead824",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 spark dataframes.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(df_lst)} spark dataframes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64b12549-0d60-4e3d-8a92-1cf873b51c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Analysing the various sizes of the datasets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c53aca71-037b-41d2-8ded-3fae050b37ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shape_df = spark.createDataFrame(\n",
    "    [\n",
    "        Row(\n",
    "            df_index=i,\n",
    "            number_of_columns=sdf.num_cols,\n",
    "            number_of_rows=sdf.num_rows,\n",
    "            file_name=sdf.name,\n",
    "        )\n",
    "        for i, sdf in enumerate(df_lst)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db13d69-8817-45c4-88b7-7ef188a931dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+--------------+----------------------------------+\n|df_index|number_of_columns|number_of_rows|file_name                         |\n+--------+-----------------+--------------+----------------------------------+\n|0       |13               |110023        |test.csv                          |\n|1       |14               |165034        |train.csv                         |\n|2       |18               |10000         |Customer_Churn_Records.csv        |\n|3       |12               |10000         |Bank_Customer_Churn_Prediction.csv|\n|4       |14               |10000         |Churn_Modeling.csv                |\n|5       |14               |10000         |Churn_Modelling.csv               |\n|6       |14               |10002         |Churn_Modelling-1.csv             |\n|7       |14               |10000         |churn.csv                         |\n+--------+-----------------+--------------+----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "shape_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b26dbf-53a2-4e7d-b9ac-e36f4e90ce9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are up to 18 columns.\n"
     ]
    }
   ],
   "source": [
    "max_num_cols = shape_df.agg({\"number_of_columns\": \"max\"}).collect()[0][0]\n",
    "print(f\"There are up to {max_num_cols} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca15c992-5901-4b37-8c08-eeb4ffb624b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Analysing the column names..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f25d8401-d7dd-4139-99b8-aa0f1de2ff23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "col_tuples = [[c for c in sdf.columns] for sdf in df_lst]\n",
    "for t in col_tuples:\n",
    "    while len(t) < shape_df.agg({\"number_of_columns\": \"max\"}).collect()[0][0]:\n",
    "        t.append(None)\n",
    "col_tuples = [[str(i)] + t for i, t in enumerate(col_tuples[:])]\n",
    "col_tuples = [tuple(t) for t in col_tuples[:]]\n",
    "\n",
    "column_pandas_df = spark.createDataFrame([Row(*t) for t in col_tuples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2aa885c-9c93-47c5-bd3e-d119d7154086",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------------+-------+-----------+---------+------+-------+---------------+-----------+-------------+----------------+--------------+---------------+------+--------+------------------+---------+------------+\n| _1|         _2|          _3|     _4|         _5|       _6|    _7|     _8|             _9|        _10|          _11|             _12|           _13|            _14|   _15|     _16|               _17|      _18|         _19|\n+---+-----------+------------+-------+-----------+---------+------+-------+---------------+-----------+-------------+----------------+--------------+---------------+------+--------+------------------+---------+------------+\n|  0|         id|  CustomerId|Surname|CreditScore|Geography|Gender|    Age|         Tenure|    Balance|NumOfProducts|       HasCrCard|IsActiveMember|EstimatedSalary|  null|    null|              null|     null|        null|\n|  1|         id|  CustomerId|Surname|CreditScore|Geography|Gender|    Age|         Tenure|    Balance|NumOfProducts|       HasCrCard|IsActiveMember|EstimatedSalary|Exited|    null|              null|     null|        null|\n|  2|  RowNumber|  CustomerId|Surname|CreditScore|Geography|Gender|    Age|         Tenure|    Balance|NumOfProducts|       HasCrCard|IsActiveMember|EstimatedSalary|Exited|Complain|Satisfaction Score|Card Type|Point Earned|\n|  3|customer_id|credit_score|country|     gender|      age|tenure|balance|products_number|credit_card|active_member|estimated_salary|         churn|           null|  null|    null|              null|     null|        null|\n|  4|  RowNumber|  CustomerId|Surname|CreditScore|Geography|Gender|    Age|         Tenure|    Balance|NumOfProducts|       HasCrCard|IsActiveMember|EstimatedSalary|Exited|    null|              null|     null|        null|\n|  5|  RowNumber|  CustomerId|Surname|CreditScore|Geography|Gender|    Age|         Tenure|    Balance|NumOfProducts|       HasCrCard|IsActiveMember|EstimatedSalary|Exited|    null|              null|     null|        null|\n|  6|  RowNumber|  CustomerId|Surname|CreditScore|Geography|Gender|    Age|         Tenure|    Balance|NumOfProducts|       HasCrCard|IsActiveMember|EstimatedSalary|Exited|    null|              null|     null|        null|\n|  7|  RowNumber|  CustomerId|Surname|CreditScore|Geography|Gender|    Age|         Tenure|    Balance|NumOfProducts|       HasCrCard|IsActiveMember|EstimatedSalary|Exited|    null|              null|     null|        null|\n+---+-----------+------------+-------+-----------+---------+------+-------+---------------+-----------+-------------+----------------+--------------+---------------+------+--------+------------------+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "column_pandas_df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871d79ac-da5d-49bd-9e19-8cc6331d7d30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What looked good in Pandas does not look good as a Spark dataframe. Approaching representation in a different angle..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3722b20-c390-40d1-89c4-dfbb0fcf1e4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_melt_df = spark.createDataFrame(\n",
    "    [\n",
    "        Row(df_index=i, column_name=c, file_name=sdf.name)\n",
    "        for i, sdf in enumerate(df_lst)\n",
    "        for c in sdf.columns\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4906771e-c1e9-4ee5-a6a6-a60814bd4495",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: 113"
     ]
    }
   ],
   "source": [
    "column_melt_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd1f2856-fe90-445e-8f7e-a1121693e51c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+----------------------------------+\n|df_index|column_name       |file_name                         |\n+--------+------------------+----------------------------------+\n|0       |id                |test.csv                          |\n|0       |CustomerId        |test.csv                          |\n|0       |Surname           |test.csv                          |\n|0       |CreditScore       |test.csv                          |\n|0       |Geography         |test.csv                          |\n|0       |Gender            |test.csv                          |\n|0       |Age               |test.csv                          |\n|0       |Tenure            |test.csv                          |\n|0       |Balance           |test.csv                          |\n|0       |NumOfProducts     |test.csv                          |\n|0       |HasCrCard         |test.csv                          |\n|0       |IsActiveMember    |test.csv                          |\n|0       |EstimatedSalary   |test.csv                          |\n|1       |id                |train.csv                         |\n|1       |CustomerId        |train.csv                         |\n|1       |Surname           |train.csv                         |\n|1       |CreditScore       |train.csv                         |\n|1       |Geography         |train.csv                         |\n|1       |Gender            |train.csv                         |\n|1       |Age               |train.csv                         |\n|1       |Tenure            |train.csv                         |\n|1       |Balance           |train.csv                         |\n|1       |NumOfProducts     |train.csv                         |\n|1       |HasCrCard         |train.csv                         |\n|1       |IsActiveMember    |train.csv                         |\n|1       |EstimatedSalary   |train.csv                         |\n|1       |Exited            |train.csv                         |\n|2       |RowNumber         |Customer_Churn_Records.csv        |\n|2       |CustomerId        |Customer_Churn_Records.csv        |\n|2       |Surname           |Customer_Churn_Records.csv        |\n|2       |CreditScore       |Customer_Churn_Records.csv        |\n|2       |Geography         |Customer_Churn_Records.csv        |\n|2       |Gender            |Customer_Churn_Records.csv        |\n|2       |Age               |Customer_Churn_Records.csv        |\n|2       |Tenure            |Customer_Churn_Records.csv        |\n|2       |Balance           |Customer_Churn_Records.csv        |\n|2       |NumOfProducts     |Customer_Churn_Records.csv        |\n|2       |HasCrCard         |Customer_Churn_Records.csv        |\n|2       |IsActiveMember    |Customer_Churn_Records.csv        |\n|2       |EstimatedSalary   |Customer_Churn_Records.csv        |\n|2       |Exited            |Customer_Churn_Records.csv        |\n|2       |Complain          |Customer_Churn_Records.csv        |\n|2       |Satisfaction Score|Customer_Churn_Records.csv        |\n|2       |Card Type         |Customer_Churn_Records.csv        |\n|2       |Point Earned      |Customer_Churn_Records.csv        |\n|3       |customer_id       |Bank_Customer_Churn_Prediction.csv|\n|3       |credit_score      |Bank_Customer_Churn_Prediction.csv|\n|3       |country           |Bank_Customer_Churn_Prediction.csv|\n|3       |gender            |Bank_Customer_Churn_Prediction.csv|\n|3       |age               |Bank_Customer_Churn_Prediction.csv|\n|3       |tenure            |Bank_Customer_Churn_Prediction.csv|\n|3       |balance           |Bank_Customer_Churn_Prediction.csv|\n|3       |products_number   |Bank_Customer_Churn_Prediction.csv|\n|3       |credit_card       |Bank_Customer_Churn_Prediction.csv|\n|3       |active_member     |Bank_Customer_Churn_Prediction.csv|\n|3       |estimated_salary  |Bank_Customer_Churn_Prediction.csv|\n|3       |churn             |Bank_Customer_Churn_Prediction.csv|\n|4       |RowNumber         |Churn_Modeling.csv                |\n|4       |CustomerId        |Churn_Modeling.csv                |\n|4       |Surname           |Churn_Modeling.csv                |\n|4       |CreditScore       |Churn_Modeling.csv                |\n|4       |Geography         |Churn_Modeling.csv                |\n|4       |Gender            |Churn_Modeling.csv                |\n|4       |Age               |Churn_Modeling.csv                |\n|4       |Tenure            |Churn_Modeling.csv                |\n|4       |Balance           |Churn_Modeling.csv                |\n|4       |NumOfProducts     |Churn_Modeling.csv                |\n|4       |HasCrCard         |Churn_Modeling.csv                |\n|4       |IsActiveMember    |Churn_Modeling.csv                |\n|4       |EstimatedSalary   |Churn_Modeling.csv                |\n|4       |Exited            |Churn_Modeling.csv                |\n|5       |RowNumber         |Churn_Modelling.csv               |\n|5       |CustomerId        |Churn_Modelling.csv               |\n|5       |Surname           |Churn_Modelling.csv               |\n|5       |CreditScore       |Churn_Modelling.csv               |\n|5       |Geography         |Churn_Modelling.csv               |\n|5       |Gender            |Churn_Modelling.csv               |\n|5       |Age               |Churn_Modelling.csv               |\n|5       |Tenure            |Churn_Modelling.csv               |\n|5       |Balance           |Churn_Modelling.csv               |\n|5       |NumOfProducts     |Churn_Modelling.csv               |\n|5       |HasCrCard         |Churn_Modelling.csv               |\n|5       |IsActiveMember    |Churn_Modelling.csv               |\n|5       |EstimatedSalary   |Churn_Modelling.csv               |\n|5       |Exited            |Churn_Modelling.csv               |\n|6       |RowNumber         |Churn_Modelling-1.csv             |\n|6       |CustomerId        |Churn_Modelling-1.csv             |\n|6       |Surname           |Churn_Modelling-1.csv             |\n|6       |CreditScore       |Churn_Modelling-1.csv             |\n|6       |Geography         |Churn_Modelling-1.csv             |\n|6       |Gender            |Churn_Modelling-1.csv             |\n|6       |Age               |Churn_Modelling-1.csv             |\n|6       |Tenure            |Churn_Modelling-1.csv             |\n|6       |Balance           |Churn_Modelling-1.csv             |\n|6       |NumOfProducts     |Churn_Modelling-1.csv             |\n|6       |HasCrCard         |Churn_Modelling-1.csv             |\n|6       |IsActiveMember    |Churn_Modelling-1.csv             |\n|6       |EstimatedSalary   |Churn_Modelling-1.csv             |\n|6       |Exited            |Churn_Modelling-1.csv             |\n|7       |RowNumber         |churn.csv                         |\n|7       |CustomerId        |churn.csv                         |\n|7       |Surname           |churn.csv                         |\n|7       |CreditScore       |churn.csv                         |\n|7       |Geography         |churn.csv                         |\n|7       |Gender            |churn.csv                         |\n|7       |Age               |churn.csv                         |\n|7       |Tenure            |churn.csv                         |\n|7       |Balance           |churn.csv                         |\n|7       |NumOfProducts     |churn.csv                         |\n|7       |HasCrCard         |churn.csv                         |\n|7       |IsActiveMember    |churn.csv                         |\n|7       |EstimatedSalary   |churn.csv                         |\n|7       |Exited            |churn.csv                         |\n+--------+------------------+----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "column_melt_df.show(113, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad7f4b8d-0394-4818-bf66-6caa98d3b61f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: 31"
     ]
    }
   ],
   "source": [
    "column_melt_df.select(\"column_name\").dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d30843bf-5689-4128-bfc4-c8feea5dbcb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n|       column_name|\n+------------------+\n|     NumOfProducts|\n|           Balance|\n|       CreditScore|\n|               Age|\n|        CustomerId|\n|         HasCrCard|\n|         Geography|\n|            Tenure|\n|           Surname|\n|            Gender|\n|    IsActiveMember|\n|                id|\n|   EstimatedSalary|\n|         RowNumber|\n|            Exited|\n|          Complain|\n|     active_member|\n|      credit_score|\n|           balance|\n|         Card Type|\n|      Point Earned|\n|Satisfaction Score|\n|       customer_id|\n|            tenure|\n|       credit_card|\n|           country|\n|               age|\n|            gender|\n|   products_number|\n|  estimated_salary|\n|             churn|\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "column_melt_df.select(\"column_name\").dropDuplicates().show(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85505d67-40f7-4a00-9d5f-aca1e9c239c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Given that there were up to 18 columns in `shape_df.show(truncate=False)`, 31 columns in `column_df.count()` seems a bit much. The line `column_df.select('column_name').dropDuplicates().show(31)` confirms this when we see duplicate column names such as \"age\" and \"Age\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffd2d2a4-73d6-4b9b-b807-8ab2d461cd7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------------+\n|df_index|column_name|file_name                 |\n+--------+-----------+--------------------------+\n|0       |Geography  |test.csv                  |\n|1       |Geography  |train.csv                 |\n|2       |Geography  |Customer_Churn_Records.csv|\n|4       |Geography  |Churn_Modeling.csv        |\n|5       |Geography  |Churn_Modelling.csv       |\n|6       |Geography  |Churn_Modelling-1.csv     |\n|7       |Geography  |churn.csv                 |\n+--------+-----------+--------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "column_melt_df.filter(\"column_name = 'Geography'\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7043d5-e92e-4d48-845d-05e49b2a52aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A lot of the datasets appear to be duplicates. Selecting the first row from each dataset yields..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48a5ceb3-9f08-4c4b-8ed0-48b52ea88c4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([165034, 15773898, 'Lucchese', 586, 'France', 'Female', 23.0, 2, 0.0, 2, 0.0, 1.0, 160976.75])\ndict_values([0, 15674932, 'Okwudilichukwu', 668, 'France', 'Male', 33.0, 3, 0.0, 2, 1.0, 0.0, 181449.97, 0])\ndict_values([1, 15634602, 'Hargrave', 619, 'France', 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88, 1, 1, 2, 'DIAMOND', 464])\ndict_values([15634602, 619, 'France', 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88, 1])\ndict_values([1, 15634602, 'Hargrave', 619, 'France', 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88, 1])\ndict_values([1, 15634602, 'Hargrave', 619, 'France', 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88, 1])\ndict_values([1, 15634602, 'Hargrave', 619, 'France', 'Female', 42.0, 2, 0.0, 1, 1, 1, 101348.88, 1])\ndict_values([1, 15634602, 'Hargrave', 619, 'France', 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88, 1])\n"
     ]
    }
   ],
   "source": [
    "for sdf in df_lst:\n",
    "    print(sdf.spark_df.first().asDict().values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08cd4794-ad07-4ccb-9d63-db764a6e0271",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Setting aside `train.csv` and `test.csv` (because clearly this data has been shuffled for ML purposes), it appears that the rest of the data seems to all be the same data. Duplicate data may come in handy when validating data during the integration process in the upcoming data and ML pipelines. Remember each dataset had approx 10,000 rows and it can be clearly seen from the first row of each dataset that they all represent the same female customer from France with credit score 619 etc. All values appear to be the same at first glance.\n",
    "\n",
    "So there may only be two groups of data instead of seven groups of data. This is quite astonishing given that the other notebook `local_eda.ipynb` working with Pandas suggested data might be in three groups with `Customer_Churn_Records.csv` being a \"unique\" dataset. Clearly this is justification to clean the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9373116c-5d4b-4eb8-b75e-029e0775b7c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def column_lowercase_cleaner(list_of_columns: list[str]) -> list:\n",
    "    return [c.lower().replace(\" \", \"\").replace(\"_\", \"\") for c in list_of_columns[:]]\n",
    "\n",
    "\n",
    "def find_non_common_columns(dataset_1: Dataset, dataset_2: Dataset) -> set[str]:\n",
    "    return set(column_lowercase_cleaner(dataset_1.columns)) ^ set(\n",
    "        column_lowercase_cleaner(dataset_2.columns)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c68dc6ed-4752-44e1-819d-ac7610b21744",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing case sensitivity from column names, there appears to be 24 unique columns, which are: numofproducts, estimatedsalary, isactivemember, tenure, customerid, churn, creditcard, activemember, hascrcard, balance, productsnumber, surname, satisfactionscore, complain, gender, pointearned, age, cardtype, id, exited, rownumber, creditscore, country, and geography.\n"
     ]
    }
   ],
   "source": [
    "raw_column_names_df = column_melt_df.select(\"column_name\").dropDuplicates().collect()\n",
    "raw_column_names_lst = [row[0] for row in raw_column_names_df]\n",
    "column_names_lowercase = list(set(column_lowercase_cleaner(raw_column_names_lst)))\n",
    "print(\n",
    "    f'After removing case sensitivity from column names, there appears to be {len(column_names_lowercase)} unique columns, which are: {\", \".join(column_names_lowercase[:-1])}, and {column_names_lowercase[-1]}.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bb1c198-b163-4574-b00a-3df607b129a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Judging from `shape_df.show(truncate=False)` and our observation that most of the data should be duplicated, we look at the anomalies `df2` (18 columns) and `df3` (12 columns) closer, using `df4` (14 columns) as a control observation. We will come back to the anomaly of `df6` (two extra rows) later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2025fd22-6ce7-4ecf-8e74-87c23cc8479d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- RowNumber: integer (nullable = true)\n |-- CustomerId: integer (nullable = true)\n |-- Surname: string (nullable = true)\n |-- CreditScore: integer (nullable = true)\n |-- Geography: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- Tenure: integer (nullable = true)\n |-- Balance: double (nullable = true)\n |-- NumOfProducts: integer (nullable = true)\n |-- HasCrCard: integer (nullable = true)\n |-- IsActiveMember: integer (nullable = true)\n |-- EstimatedSalary: double (nullable = true)\n |-- Exited: integer (nullable = true)\n |-- Complain: integer (nullable = true)\n |-- Satisfaction Score: integer (nullable = true)\n |-- Card Type: string (nullable = true)\n |-- Point Earned: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_lst[2].spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb778430-86b1-4263-9e02-cd252977a9fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- customer_id: integer (nullable = true)\n |-- credit_score: integer (nullable = true)\n |-- country: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- tenure: integer (nullable = true)\n |-- balance: double (nullable = true)\n |-- products_number: integer (nullable = true)\n |-- credit_card: integer (nullable = true)\n |-- active_member: integer (nullable = true)\n |-- estimated_salary: double (nullable = true)\n |-- churn: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_lst[3].spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67271e31-e2c2-4252-80ee-cf30f69a15f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- RowNumber: integer (nullable = true)\n |-- CustomerId: integer (nullable = true)\n |-- Surname: string (nullable = true)\n |-- CreditScore: integer (nullable = true)\n |-- Geography: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- Tenure: integer (nullable = true)\n |-- Balance: double (nullable = true)\n |-- NumOfProducts: integer (nullable = true)\n |-- HasCrCard: integer (nullable = true)\n |-- IsActiveMember: integer (nullable = true)\n |-- EstimatedSalary: double (nullable = true)\n |-- Exited: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_lst[4].spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b73da30c-9b59-429e-b12e-59761b892d71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[23]: {'activemember',\n 'churn',\n 'country',\n 'creditcard',\n 'exited',\n 'geography',\n 'hascrcard',\n 'isactivemember',\n 'numofproducts',\n 'productsnumber',\n 'rownumber',\n 'surname'}"
     ]
    }
   ],
   "source": [
    "find_non_common_columns(df_lst[3], df_lst[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18739190-eca9-4cce-9e15-bbf5f6882713",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It appears that `df3` is identical to `df4` where `df3` drops columns `rownumber` and `surname`. We can also see some unexpected pairings that contribute to extra \"unique\" columns:\n",
    "\n",
    "- `country` and `geography`\n",
    "- `numofproducts` and `productsnumber`\n",
    "- `creditcard` and `hascrcard`\n",
    "- `activemember` and `isactivemember`\n",
    "- `churn` and `exited`\n",
    "\n",
    "We also see that `df3` has wildly different naming. So we compare `df2` and `df4` rather than comparing `df2` and `df3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0515ebf-41a1-4e76-85f4-01c0d62b0431",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: {'cardtype', 'complain', 'pointearned', 'satisfactionscore'}"
     ]
    }
   ],
   "source": [
    "find_non_common_columns(df_lst[2], df_lst[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62dc7b5-b02b-44d8-b9f6-1aabc97646ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And we confirm that `df2` has the exact same columns, where the only difference is that four new columns have been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a326d7-cc2a-4c84-afa0-f00b08ea84f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RowNumber': 1, 'CustomerId': 15634602, 'Surname': 'Hargrave', 'CreditScore': 619, 'Geography': 'France', 'Gender': 'Female', 'Age': 42, 'Tenure': 2, 'Balance': 0.0, 'NumOfProducts': 1, 'HasCrCard': 1, 'IsActiveMember': 1, 'EstimatedSalary': 101348.88, 'Exited': 1, 'Complain': 1, 'Satisfaction Score': 2, 'Card Type': 'DIAMOND', 'Point Earned': 464}\n{'customer_id': 15634602, 'credit_score': 619, 'country': 'France', 'gender': 'Female', 'age': 42, 'tenure': 2, 'balance': 0.0, 'products_number': 1, 'credit_card': 1, 'active_member': 1, 'estimated_salary': 101348.88, 'churn': 1}\n{'RowNumber': 1, 'CustomerId': 15634602, 'Surname': 'Hargrave', 'CreditScore': 619, 'Geography': 'France', 'Gender': 'Female', 'Age': 42, 'Tenure': 2, 'Balance': 0.0, 'NumOfProducts': 1, 'HasCrCard': 1, 'IsActiveMember': 1, 'EstimatedSalary': 101348.88, 'Exited': 1}\n"
     ]
    }
   ],
   "source": [
    "filtered_df_lst = [df_lst[i] for i in [2, 3, 4]]\n",
    "for sdf in filtered_df_lst:\n",
    "    print(sdf.spark_df.first().asDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094337ab-8efa-456d-b416-7afd42831e5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally, we check the schema for consistent data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb96155d-0a55-402a-873d-2017c5018b38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df0 shape(110023, 13): test.csv\nroot\n |-- id: integer (nullable = true)\n |-- CustomerId: integer (nullable = true)\n |-- Surname: string (nullable = true)\n |-- CreditScore: integer (nullable = true)\n |-- Geography: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- Tenure: integer (nullable = true)\n |-- Balance: double (nullable = true)\n |-- NumOfProducts: integer (nullable = true)\n |-- HasCrCard: double (nullable = true)\n |-- IsActiveMember: double (nullable = true)\n |-- EstimatedSalary: double (nullable = true)\n\ndf1 shape(165034, 14): train.csv\nroot\n |-- id: integer (nullable = true)\n |-- CustomerId: integer (nullable = true)\n |-- Surname: string (nullable = true)\n |-- CreditScore: integer (nullable = true)\n |-- Geography: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- Tenure: integer (nullable = true)\n |-- Balance: double (nullable = true)\n |-- NumOfProducts: integer (nullable = true)\n |-- HasCrCard: double (nullable = true)\n |-- IsActiveMember: double (nullable = true)\n |-- EstimatedSalary: double (nullable = true)\n |-- Exited: integer (nullable = true)\n\ndf2 shape(10000, 18): Customer_Churn_Records.csv\nroot\n |-- RowNumber: integer (nullable = true)\n |-- CustomerId: integer (nullable = true)\n |-- Surname: string (nullable = true)\n |-- CreditScore: integer (nullable = true)\n |-- Geography: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- Tenure: integer (nullable = true)\n |-- Balance: double (nullable = true)\n |-- NumOfProducts: integer (nullable = true)\n |-- HasCrCard: integer (nullable = true)\n |-- IsActiveMember: integer (nullable = true)\n |-- EstimatedSalary: double (nullable = true)\n |-- Exited: integer (nullable = true)\n |-- Complain: integer (nullable = true)\n |-- Satisfaction Score: integer (nullable = true)\n |-- Card Type: string (nullable = true)\n |-- Point Earned: integer (nullable = true)\n\ndf3 shape(10000, 12): Bank_Customer_Churn_Prediction.csv\nroot\n |-- customer_id: integer (nullable = true)\n |-- credit_score: integer (nullable = true)\n |-- country: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- tenure: integer (nullable = true)\n |-- balance: double (nullable = true)\n |-- products_number: integer (nullable = true)\n |-- credit_card: integer (nullable = true)\n |-- active_member: integer (nullable = true)\n |-- estimated_salary: double (nullable = true)\n |-- churn: integer (nullable = true)\n\ndf4 shape(10000, 14): Churn_Modeling.csv\nroot\n |-- RowNumber: integer (nullable = true)\n |-- CustomerId: integer (nullable = true)\n |-- Surname: string (nullable = true)\n |-- CreditScore: integer (nullable = true)\n |-- Geography: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- Tenure: integer (nullable = true)\n |-- Balance: double (nullable = true)\n |-- NumOfProducts: integer (nullable = true)\n |-- HasCrCard: integer (nullable = true)\n |-- IsActiveMember: integer (nullable = true)\n |-- EstimatedSalary: double (nullable = true)\n |-- Exited: integer (nullable = true)\n\ndf5 shape(10000, 14): Churn_Modelling.csv\nroot\n |-- RowNumber: integer (nullable = true)\n |-- CustomerId: integer (nullable = true)\n |-- Surname: string (nullable = true)\n |-- CreditScore: integer (nullable = true)\n |-- Geography: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- Tenure: integer (nullable = true)\n |-- Balance: double (nullable = true)\n |-- NumOfProducts: integer (nullable = true)\n |-- HasCrCard: integer (nullable = true)\n |-- IsActiveMember: integer (nullable = true)\n |-- EstimatedSalary: double (nullable = true)\n |-- Exited: integer (nullable = true)\n\ndf6 shape(10002, 14): Churn_Modelling-1.csv\nroot\n |-- RowNumber: integer (nullable = true)\n |-- CustomerId: integer (nullable = true)\n |-- Surname: string (nullable = true)\n |-- CreditScore: integer (nullable = true)\n |-- Geography: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- Tenure: integer (nullable = true)\n |-- Balance: double (nullable = true)\n |-- NumOfProducts: integer (nullable = true)\n |-- HasCrCard: integer (nullable = true)\n |-- IsActiveMember: integer (nullable = true)\n |-- EstimatedSalary: double (nullable = true)\n |-- Exited: integer (nullable = true)\n\ndf7 shape(10000, 14): churn.csv\nroot\n |-- RowNumber: integer (nullable = true)\n |-- CustomerId: integer (nullable = true)\n |-- Surname: string (nullable = true)\n |-- CreditScore: integer (nullable = true)\n |-- Geography: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- Tenure: integer (nullable = true)\n |-- Balance: double (nullable = true)\n |-- NumOfProducts: integer (nullable = true)\n |-- HasCrCard: integer (nullable = true)\n |-- IsActiveMember: integer (nullable = true)\n |-- EstimatedSalary: double (nullable = true)\n |-- Exited: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "for i, sdf in enumerate(df_lst):\n",
    "    print(f\"df{i} shape({sdf.num_rows}, {sdf.num_cols}): {sdf.name}\")\n",
    "    sdf.spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "108d729e-44d8-4678-b8bf-4b13ab0aa4d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In conclusion, there are 2 datasets and up to 18 columns. A data pipeline is needed to fix consistency between columns names and data types (eg `Age` is a double in `df6` but an integer in `df7`)."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Exploratory Data Analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
