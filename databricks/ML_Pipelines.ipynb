{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0825622a-dfc3-4c54-b81d-528f02f0af2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Machine Learning Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "673f21a9-78eb-409a-a1b4-d8ec21e7bc81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Various classifiers from PySpark's ml library were selected as candidates to fit the data concerning customer churn since the data pipeline was already hosted on PySpark, and the customer churn problem appears to be a binary classification problem. PySpark appears to be convenient to use in this case, so the plan is to build ml models in PySpark to fit and predict customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e81013bb-c6da-4253-955b-8ac6f2c8a203",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructField\n",
    "    , StringType\n",
    "    , IntegerType\n",
    "    , DoubleType\n",
    "    , BooleanType\n",
    "    , StructType\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler\n",
    "    , OneHotEncoder\n",
    "    , StringIndexer\n",
    "    , Imputer\n",
    ")\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression\n",
    "    , DecisionTreeClassifier\n",
    "    , GBTClassifier\n",
    "    , RandomForestClassifier\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import (\n",
    "    BinaryClassificationEvaluator\n",
    "    , MulticlassClassificationEvaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2246e734-4ad6-4acf-aa16-ca6c7625c3a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ML_Pipeline').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "570ce6c6-3ffc-4884-bd3c-a87f54fdcb18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- customer_id: integer (nullable = true)\n |-- surname: string (nullable = true)\n |-- credit_score: integer (nullable = true)\n |-- geography: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- tenure: integer (nullable = true)\n |-- balance: double (nullable = true)\n |-- product_count: integer (nullable = true)\n |-- has_creditcard: integer (nullable = true)\n |-- active_member: integer (nullable = true)\n |-- estimated_salary: double (nullable = true)\n |-- complain: integer (nullable = true)\n |-- satisfaction_score: integer (nullable = true)\n |-- card_type: string (nullable = true)\n |-- points_earned: integer (nullable = true)\n |-- churn: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "my_schema = StructType(fields=[\n",
    "    StructField('customer_id', IntegerType(), True)\n",
    "    , StructField('surname', StringType(), True)\n",
    "    , StructField('credit_score', IntegerType(), True)\n",
    "    , StructField('geography', StringType(), True)\n",
    "    , StructField('gender', StringType(), True)\n",
    "    , StructField('age', IntegerType(), True)\n",
    "    , StructField('tenure', IntegerType(), True)\n",
    "    , StructField('balance', DoubleType(), True)\n",
    "    , StructField('product_count', IntegerType(), True)\n",
    "    , StructField('has_creditcard', IntegerType(), True)\n",
    "    , StructField('active_member', IntegerType(), True)\n",
    "    , StructField('estimated_salary', DoubleType(), True)\n",
    "    , StructField('complain', IntegerType(), True)\n",
    "    , StructField('satisfaction_score', IntegerType(), True)\n",
    "    , StructField('card_type', StringType(), True)\n",
    "    , StructField('points_earned', IntegerType(), True)\n",
    "    , StructField('churn', IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = spark.read.csv(\n",
    "    '/FileStore/tables/pyspark_churn.csv'\n",
    "    , header=True\n",
    "    , schema=my_schema\n",
    ")\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c0efc43-05a1-41d9-be71-1762f947530e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "During a testing phase, many errors were thrown from the ml objects due to a lack of data. Further cleaning shows that many columns held no data. The columns were either entirely null or contained only a single value. These are removed so the ml objects can accept meaningful data. Nulls are also dropped and the data is split into a train-test split. From a prior EDA document, a third category `unlabelled_data` is also created as this accounts for 5/13 of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab49166-ee9e-4b1b-ad8b-fb1551f6e884",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column age has no data.\nColumn has_creditcard has no data.\nColumn active_member has no data.\nColumn complain has no data.\nColumn satisfaction_score has no data.\nColumn card_type has no data.\nColumn points_earned has no data.\n\nThe left over columns are features\n['credit_score', 'geography', 'gender', 'tenure', 'balance', 'product_count', 'estimated_salary']\nand label ['churn'].\n\n"
     ]
    }
   ],
   "source": [
    "kept_columns = []\n",
    "for c in data.columns:\n",
    "    if data.select(c).distinct().count() <= 1 or data.select(c).na.drop().count() == 0:\n",
    "        print(f'Column {c} has no data.')\n",
    "    else:\n",
    "        kept_columns.append(c)\n",
    "label = 'churn'\n",
    "raw_features = [c for c in kept_columns if c not in {'customer_id', 'surname', label}]\n",
    "ml_columns = raw_features + [label]\n",
    "print(f'''\n",
    "The left over columns are features\n",
    "{raw_features}\n",
    "and label {[label]}.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "896ef6d2-aba2-4193-ad42-0b60baf241be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls have been dropped.\n"
     ]
    }
   ],
   "source": [
    "nonnull_count = data.select(ml_columns).na.drop().count()\n",
    "if nonnull_count < 1000:\n",
    "    print(f'There are {nonnull_count} data points after dropping nulls. An imputer is required.')\n",
    "else:\n",
    "    data = data.na.drop(how='all')\n",
    "    print('Nulls have been dropped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16ba9cdf-8bb0-492f-bb90-635742e1dd69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total customers: 13545\nTracked customers: 8430\nNew customers: 5115\n"
     ]
    }
   ],
   "source": [
    "labelled_data = data.select(ml_columns).filter(data.churn.isNotNull())\n",
    "unlabelled_data = data.select(ml_columns).filter(data.churn.isNull())\n",
    "print(f'Total customers: {data.count()}')\n",
    "print(f'Tracked customers: {labelled_data.count()}')\n",
    "print(f'New customers: {unlabelled_data.count()}')\n",
    "train, test = labelled_data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10fae64c-f998-427c-a846-84843d5fbf17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A final check shows that each columns has at least 2 distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5179c964-54ba-40f3-8241-b917de4ad0fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column credit_score has 365 distinct values.\nColumn geography has 3 distinct values.\nColumn gender has 2 distinct values.\nColumn tenure has 11 distinct values.\nColumn balance has 3240 distinct values.\nColumn product_count has 4 distinct values.\nColumn estimated_salary has 6276 distinct values.\nColumn churn has 2 distinct values.\n"
     ]
    }
   ],
   "source": [
    "for c in train.columns:\n",
    "    population = train.select(c).distinct().count()\n",
    "    assert population>1, f'The ml models will not work on this data: column {c} is invalid.'\n",
    "    print(f'Column {c} has {population} distinct values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d0d2a8-5a1a-4070-a58b-83f255d8ad74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- credit_score: integer (nullable = true)\n |-- geography: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- tenure: integer (nullable = true)\n |-- balance: double (nullable = true)\n |-- product_count: integer (nullable = true)\n |-- estimated_salary: double (nullable = true)\n |-- churn: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccfcab06-5d06-4ad7-b779-0d626b96204a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Columns `geography` and `gender` are strings which will not be accepted by ml objects. This issue can be swiftly fixed by forcing a numerical value on them by way of an indexer. The result is one-hot-encoded to reflect that this is categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9da7ef45-767b-4466-8814-f7065d6c927c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_columns = ['geography', 'gender']\n",
    "feature_columns = [c if c not in categorical_columns else c+'_onehotencode' for c in raw_features]\n",
    "\n",
    "geography_indexer = StringIndexer(inputCol='geography',outputCol='geography_index')\n",
    "geography_encoder = OneHotEncoder(inputCol='geography_index',outputCol='geography_onehotencode')\n",
    "gender_indexer = StringIndexer(inputCol='gender',outputCol='gender_index')\n",
    "gender_encoder = OneHotEncoder(inputCol='gender_index',outputCol='gender_onehotencode')\n",
    "assembler = VectorAssembler(inputCols=feature_columns,outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc8c2182-4c00-4b95-b219-236b4505e5dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A dictionary holds the names and objects of the proposed classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "320fa00d-282f-4a9c-9fb4-71cbfd83d3dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ml_objects = {\n",
    "    'logistic_regression': LogisticRegression(featuresCol='features',labelCol='churn')\n",
    "    , 'decision_tree': DecisionTreeClassifier(featuresCol='features',labelCol='churn')\n",
    "    , 'gbt': GBTClassifier(featuresCol='features',labelCol='churn')\n",
    "    , 'random_forest': RandomForestClassifier(featuresCol='features',labelCol='churn')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e2f0f31-f0bc-4115-826f-42de997170af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Some common sample metrics were selected from the confusion matrix in order to compare the different models with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a308cb8-7434-4ee9-a4ce-f23fe8fcd9cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "auc_eval = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='prediction'\n",
    "    , labelCol='churn'\n",
    ")\n",
    "acc_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol='churn'\n",
    "    , predictionCol='prediction'\n",
    "    , metricName='accuracy'\n",
    ")\n",
    "precision_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol='churn'\n",
    "    , predictionCol='prediction'\n",
    "    , metricName='weightedPrecision'\n",
    ")\n",
    "recal_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol='churn'\n",
    "    , predictionCol='prediction'\n",
    "    , metricName='weightedRecall'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ba72d89-a9a5-43fd-8521-b3924869882e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Everything is put together at the end: creating each ml pipeline, feeding it the training data, and evaluating the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f249314-6e39-4220-bee4-b4cb1f667583",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logistic_regression model has...\nAUC: 0.5818180183096818\nAccuracy: 0.7910714285714285\nPrecision: 0.7678548022354738\nRecall: 0.7910714285714285\n\nThe decision_tree model has...\nAUC: 0.689098125266042\nAccuracy: 0.8196428571428571\nPrecision: 0.8055968845025709\nRecall: 0.8196428571428571\n\nThe gbt model has...\nAUC: 0.6813461872815458\nAccuracy: 0.8160714285714286\nPrecision: 0.8010094019612761\nRecall: 0.8160714285714286\n\nThe random_forest model has...\nAUC: 0.5492275040918002\nAccuracy: 0.7910714285714285\nPrecision: 0.8200320512820514\nRecall: 0.7910714285714286\n\n"
     ]
    }
   ],
   "source": [
    "saved_models = dict()\n",
    "for name, ml_obj in ml_objects.items():\n",
    "    pipeline = Pipeline(stages=[\n",
    "        geography_indexer\n",
    "        , gender_indexer\n",
    "        , geography_encoder\n",
    "        , gender_encoder\n",
    "        , assembler\n",
    "        , ml_obj\n",
    "    ])\n",
    "    model = pipeline.fit(train)\n",
    "    saved_models[name] = model\n",
    "    result = model.transform(test)\n",
    "    auc = auc_eval.evaluate(result)\n",
    "    accuracy = acc_eval.evaluate(result)\n",
    "    precision = precision_eval.evaluate(result)\n",
    "    recall = recal_eval.evaluate(result)\n",
    "    print(f'The {name} model has...')\n",
    "    print(f'AUC: {auc}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cfcbcf8-074e-42a1-90aa-a059f339b4ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "The \"best\" model can be selected and used on new customers. Here, the `decision_tree` classifier is selected because it has the highest accuracy. Optionally, the predictions may be saved to a database of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a33960cb-aa7a-4730-81eb-94dee7366479",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = saved_models['decision_tree']\n",
    "predictions = model.transform(unlabelled_data)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ML_Pipelines",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
